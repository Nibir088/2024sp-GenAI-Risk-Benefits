---
layout: post
title: FM fairness / bias issues 
lecture: S0-Intro
lectureVersion: next
extraContent: 
notes: team-2
video: team-5
tags:
- 1Basic
---

In this session, our readings cover: 

## Required Readings: 

### Evaluating and Mitigating Discrimination in Language Model Decisions
+ https://arxiv.org/abs/2312.03689
+ As language models (LMs) advance, interest is growing in applying them to high-stakes societal decisions, such as determining financing or housing eligibility. However, their potential for discrimination in such contexts raises ethical concerns, motivating the need for better methods to evaluate these risks. We present a method for proactively evaluating the potential discriminatory impact of LMs in a wide range of use cases, including hypothetical use cases where they have not yet been deployed. Specifically, we use an LM to generate a wide array of potential prompts that decision-makers may input into an LM, spanning 70 diverse decision scenarios across society, and systematically vary the demographic information in each prompt. Applying this methodology reveals patterns of both positive and negative discrimination in the Claude 2.0 model in select settings when no interventions are applied. While we do not endorse or permit the use of language models to make automated decisions for the high-risk use cases we study, we demonstrate techniques to significantly decrease both positive and negative discrimination through careful prompt engineering, providing pathways toward safer deployment in use cases where they may be appropriate. Our work enables developers and policymakers to anticipate, measure, and address discrimination as language model capabilities and applications continue to expand. We release our dataset and prompts at this https URL


## More Readings: 

### Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models
 + https://arxiv.org/abs/2310.11079

### Machine Learning in development: Let's talk about bias! 
  + https://huggingface.co/blog/ethics-soc-2 
  + https://huggingface.co/blog/evaluating-llm-bias 

### Exploring Social Bias in Chatbots using Stereotype Knowledge WNLP@ACL2019

### Bias and Fairness in Large Language Models: A Survey
  + https://arxiv.org/abs/2309.00770
  + Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly-available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.

### A Survey on Fairness in Large Language Models
  + https://arxiv.org/abs/2308.10149
  + Large language models (LLMs) have shown powerful performance and development prospect and are widely deployed in the real world. However, LLMs can capture social biases from unprocessed training data and propagate the biases to downstream tasks. Unfair LLM systems have undesirable social impacts and potential harms. In this paper, we provide a comprehensive review of related research on fairness in LLMs. First, for medium-scale LLMs, we introduce evaluation metrics and debiasing methods from the perspectives of intrinsic bias and extrinsic bias, respectively. Then, for large-scale LLMs, we introduce recent fairness research, including fairness evaluation, reasons for bias, and debiasing methods. Finally, we discuss and provide insight on the challenges and future directions for the development of fairness in LLMs.
  
# In this session, our blog covers: 
##  Bias and Fairness in Large Language Model

### 1 &nbsp; &nbsp; Formal Definition of Bias and Fairness (LLM context)
#### 1.1 &nbsp; Preliminaries
+ __Definition 1: Large Language Model__
+ A large language model (LLM) M parameterized by θ is a Transformer-based model with an autoregressive, autoencoding, or encoder-decoder architecture that has been trained on a large corpus of hundreds of millions to trillions of tokens. LLMs encompass pre-trained models.
+ __Definition 2: Evaluation Metric__
+ For some evaluation dataset (D) there exists a subset of metrics ψ(D) (from space of all metrics Ψ) that are appropriate for D 
<img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/screenshot1.jpg" width="50%" height="50%">

#### 1.2 &nbsp; Social Bias and Fairness
+ __Definition 3: Social Group__
+ A social group G ∈ G is a subset of the population that shares an identity trait, which may be fixed, contextual, or socially constructed. Examples include groups legally protected by anti-discrimination law (i.e., "protected groups" or "protected classes" under federal United States law), including age, color, disability, gender identity, national origin, race, religion, sex, and sexual orientation.
+ __Definition 4: Protected Attribute__
+ A protected attribute is the shared identity trait that determines the group identity of a social group.
+ __Definition 5: Group Fairness__
+ Consider a model M and an outcome Yˆ = M(X;θ). Given a set of social groups G, group fairness requires (approximate) parity across all groups G ∈ G, up to ε, of a statistical outcome measure MY (G) conditioned on group membership:
<p style="text-align: center;">|MY (G) − MY (G′)| ≤ ε</p>
<img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/screenshot2.jpg" width="50%" height="50%">
+ __Definition 6: Individual Fairness__
+ Consider two individuals x, x′ ∈ V and a distance metric d : V × V → R. Let O be the set of outcomes, and let M : V → ∆(O) be a transformation from an individual to a distribution over outcomes. Individual fairness requires that individuals similar with respect to some task should be treated similarly, such that
<p style="text-align: center;">∀x, x′ ∈ V. D (M(x), M(x′)) ≤ d(x, x′)</p>
+ where D is some measure of similarity between distributions, such as statistical distance.
<img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/screenshot3.jpg" width="50%" height="50%">
+ __Definition 7: Social Bias__
+ Social bias broadly encompasses disparate treatment or outcomes between social groups that arise from historical and structural power asymmetries.
<img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/table1.jpg" width="50%" height="50%">

#### 1.3 &nbsp; Bias in NLP Tasks
__Text Generation__
+ Predicting next token : ‘The man was known for [BLANK]" versus "The woman was known for [BLANK].’
+ __Machine Translation__
+ Translation defaults to masculine words:
+ English: “I am happy”
+ French: "je suis heureux" masculine more often as opposed to the feminine form "je suis heureuse" 
+ __Information Retrieval__
+ Retrieved documents have more masculine-related concepts instead of feminine
+ __Question-Answering__
+ Model relies on stereotypes to answer questions. (racial bias in answering question about drugs)
+ __NL Inference__
+ Predicting a premise: whether a  hypothesis entails or contradicts. Make invalid inference.
+ ACTUAL: the accountant ate a bagel“
+ WRONG: the man ate a bagel" or "the woman ate a bagel
+ __Classification__
+ Toxicity Models misclassify African American tweets as negative more often then in Standard American English

#### 1.4 &nbsp; Fairness Constraints
<img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/screenshot4.jpg" width="50%" height="50%">

### 2 &nbsp; &nbsp; Taxonomy of Metrics used to evaluate Bias 
#### 2.1 &nbsp; Facets of Metrics
+ __Task Specific __
+ Different NLP task type (text generation, classification etc.) need different metrics.
+ __Bias Type __
+ Bias type varies between datasets so metrics might change.
+ __Data structure (input to model)__
+ e.g.: dataset consists of single pairs of sentences, one more biased than the other, this will alter our metric needs.
+ __Data Structure (output from model)__
+ Output type can change metric. Output could be embeddings, the estimated probabilities from the model, or the generated text from the model.

#### 2.2 &nbsp; Taxonomy of Metrics based on What They Use1. 
+ __Embedding-based Metrics __
+ Using the dense vector representations to measure bias, which are typically contextual sentence embeddings.
+ __Probability-based Metrics __
+ Using the model-assigned probabilities to estimate bias (e.g., to score text pairs or answer multiple-choice questions).
+ __Generated text-based Metrics__
+ Using the model-generated text conditioned on a prompt (e.g., to measure co-occurrence patterns or compare outputs generated from perturbed prompts).

#### 2.3 &nbsp; Embedding-based Metrics
+ __Word Embedding Metrics__
+ After encoder has generated vectors from words: We see how bias can shift certain words closer to others
+ __WEAT (pre-LLM NLP era): __WEAT measures associations between social group concepts (e.g., masculine and feminine words) and neutral attributes (e.g., family and occupation words). For protected attributes A1, A2 and neutral words W1 and W2. We define test statistic f:
<img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/screenshot5.jpg" width="50%" height="50%">
+ __Sentence Embedding Metrics__
Instead of using static word embeddings, LLMs use embeddings learned in the context of a sentence, and are more appropriately paired with embedding metrics for sentence-level encoders. Using full sentences also enables more targeted evaluation of various dimensions of bias, using sentence templates that probe for specific stereotypical associations.
+ __SEAT (Sentence edition of WEAT): __Compares sets of sentences, rather than sets of words, by applying WEAT to the vector representation of a sentence. 
<img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/screenshot6.jpg" width="50%" height="50%">
+ __Problems of Embedding-based metrics__
+ Several works point out that biases in the embedding space have only weak or inconsistent relationships with biases in downstream tasks (Cabello et al., 2023; Cao et al., 2022; Goldfarb-Tarrant et al., 2021; Orgad & Belinkov, 2022; Orgad et al., 2022; Steed et al., 2022).
+ Goldfarb-Tarrant et al. (2021) find no reliable correlation at all, and Cabello et al. (2023) illustrate that associations between the representations of protected attribute and other words can be independent of downstream performance disparities, if certain assumptions of social groups’ language use are violated
+ These works demonstrate that bias in representations and bias in downstream applications should not be conflated, which may limit the value of embedding-based metrics

#### 2.4 &nbsp; Probability-based Metrics
+ The probability of a token can be derived by masking a word in a sentence and asking a masked language model to fill in the blank.
<img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/picture2.jpg" width="50%" height="50%">
<img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/screenshot7.jpg" width="50%" height="50%">
+ PLL should be same for both cases for it to be unbiased.
+ __Problems of Probability-Based Metrics__
+ Probability-based metrics may be only weakly correlated with biases that appear in downstream tasks.
+ Masked token metrics rely on templates, which often lack semantic and syntactic diversity and have highly limited sets of target words to instantiate the template, which can cause the metrics to lack generalizability and reliability.
+ Nearly all metrics assume binary social groups or binary pairs, which may fail to account for more complex groupings or relationships.

#### 2.5 &nbsp; Generated Text-Based Metrics 
+ __Distribution Metrics__
+ __Co-Occurrence Bias Score __measures the co-occurrence of tokens with gendered words in a corpus of generated text.
<img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/screenshot8.jpg" width="50%" height="50%">
+ If score = 0, masculine and feminine words have equally likely chance of appearing.
+ __Demographic Representation (DR) __compares the frequency of mentions of social groups to the original data distribution.
<img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/screenshot9.jpg" width="50%" height="50%">

+ __Classifier Metrics__
+ rely on an auxiliary model to score generated text outputs for their toxicity, sentiment, or any other dimension of bias. Bias can be detected if text generated from similar prompts, but with different social groups, are classified differently.
+ __Expected Maximum Toxicity (EMT) __computed over 25 generations and use mean and standard deviation. This metric characterizes the worst-case generations of an LLM. For a toxicity detector c : Y → [0, 1]
<img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/picture3.jpg" width="50%" height="50%">
+ __Toxicity Probability (TP) __measures the empirical probability of generating at least one text with a toxicity score ≥ 0.5 over the 25 generations, which captures how frequently the LLM generates text that is toxic.
<img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/picture4.jpg" width="50%" height="50%">

+ __Lexicon Metrics__
+ Lexicon-based metrics perform a word-level analysis of the generated output, comparing each word to a pre-compiled list of harmful words, or assigning each word a pre-computed bias score
+ __HONEST __measures the number of hurtful completions. 
<img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/screenshot10.jpg" width="50%" height="50%">

+ __Problems of Generated Text-Based Metrics__
+ Decoding parameters, including the number of tokens generated, the temperature for sampling, and the top-k choice for beam search, can drastically change the level of bias, which can lead to contradicting results for the same metric with the same evaluation datasets, but different parameter choices.
+ Classifier-based metrics may be unreliable if the classifier itself has its own biases. (Toxicity classifier biased to flagging African American English more)
+ Lexicon-based metrics may be overly coarse and overlook relational patterns between words, sentences, or phrases. 



 
## Prompt Designing: Mitigation Techniques

__Prompt interventions__

Mitigation in decision-making step

  <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/1.png" width="100%" height="100%">

---

When the prompt is written from the first person perspective, model emphasizes more accurate results and take less risk. Biases are injected through data. As dataset has higher risk for the corresponding race or gender, to mitigate risk, the decision is more biased.
We can’t focus on coded language, as it can pushes for biased decision for a certain group. 

__Prompt interventions__

Mitigation in decision-making step

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/2.png" width="100%" height="100%">

---

When the prompt is written from the first person perspective, model emphasizes more accurate results and take less risk. Biases are injected through data. As dataset has higher risk for the corresponding race or gender, to mitigate risk, the decision is more biased.
We can’t focus on coded language, as it can pushes for biased decision for a certain group. 

__Prompt interventions__

Mitigation in decision-making step

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/3.png" width="100%" height="100%">

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/4.png" width="100%" height="100%">

---

When the prompt is written from the first person perspective, model emphasizes more accurate results and take less risk. Biases are injected through data. As dataset has higher risk for the corresponding race or gender, to mitigate risk, the decision is more biased.
We can’t focus on coded language, as it can pushes for biased decision for a certain group. 

Prompt interventions

__Mitigation in decision-making step__

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/5.png" width="100%" height="100%">

---

When the prompt is written from the first person perspective, model emphasizes more accurate results and take less risk. Biases are injected through data. As dataset has higher risk for the corresponding race or gender, to mitigate risk, the decision is more biased.
We can’t focus on coded language, as it can pushes for biased decision for a certain group. 

Prompt interventions

__Mitigation in decision-making step__

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/6.png" width="100%" height="100%">

---

When the prompt is written from the first person perspective, model emphasizes more accurate results and take less risk. Biases are injected through data. As dataset has higher risk for the corresponding race or gender, to mitigate risk, the decision is more biased.
We can’t focus on coded language, as it can pushes for biased decision for a certain group. 

## Prompt intervention mitigates discrimination!

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/7.png" width="100%" height="100%">

---

Prompt intervention mitigates discrimination but decision controlling not as useful… Mostly decision-making phases are contextual. Biases is not defined explicitly. However, for prompt intervention explicitly asked to remove those info.

<span style="color:#FF0000">Not significant improvement</span>

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/8.png" width="100%" height="100%">
---

Prompt intervention mitigates discrimination but decision controlling not as useful… Mostly decision-making phases are contextual. Biases is not defined explicitly. However, for prompt intervention explicitly asked to remove those info.

<span style="color:#00B050">Noticeable improvement</span>

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/9.png" width="100%" height="100%">

---

Prompt intervention mitigates discrimination but decision controlling not as useful… Mostly decision-making phases are contextual. Biases is not defined explicitly. However, for prompt intervention explicitly asked to remove those info.

## Does the intervention distort the model decision?

<span style="color:#FF0000">Does it make decision of the model less useful?</span>

Ask for decision

Find correlation

<span style="color:#FF0000">Ask for decision</span>

Decision with intervention

---

Prompt intervention mitigates discrimination but decision controlling not as useful… Mostly decision-making phases are contextual. Biases is not defined explicitly. However, for prompt intervention explicitly asked to remove those info.

## Intervention maintains a high correlation with the original decision

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/10.png" width="100%" height="100%">

---

Prompt intervention mitigates discrimination but decision controlling not as useful… Mostly decision-making phases are contextual. Biases is not defined explicitly. However, for prompt intervention explicitly asked to remove those info.

## Evaluation Limitation

* Specific set of prompts
  * People may use wide variety of prompts
  * Do not use wide range of characteristics
    * Veteran status, income, health status
    * Selection of names
    * Consider only LLM’s decision
    * Do not consider intersectional effects
      * Race and age
      * Sensitivity study should be on larger scale

---

Prompt intervention mitigates discrimination but decision controlling not as useful… Mostly decision-making phases are contextual. Biases is not defined explicitly. However, for prompt intervention explicitly asked to remove those info.



## Presentation Outline

Evaluating and Mitigating Discrimination in Language Model Decisions

Bias and Fairness Evaluation

<span style="color:#00B050">Gender Bias in LLM</span>

__Gender Bias Provocation and Mitigation in LLMs__

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/12.png" width="100%" height="100%">

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/13.png" width="100%" height="100%">

_Traditional biases investigation methods:_  Rely on human-written test cases

<span style="color:#C00000">Expensive</span>

<span style="color:#C00000">Limited</span>

Example of test cases and responses of  _Alpaca_  before and after mitigation

_Introducing a new mitigation strategy:_

Automatically generates test cases to detect LLMs’ potential gender bias.

__Related Previous Work__

_Bias Mitigation in Natural Language Generation_

_Bias Investigation in Natural Language Generation_

Local bias-based

Global bias-based

Mainly aim to reduce bias by replacing or deleting biased words in training data

Hand-crafted templates to evaluate bias

Adversarial Learning8,9 which fine-tunes the model using an adversarial loss to eliminate bias.

Concept of Null space projection10 to eliminate gender features in models

Multiple classifiers to evaluate bias by comparing the classification results of generated texts from various perspectives

Counterfactual Data Augmentation (CDA)

For example, the template can be a sentence with some masked words. We can then evaluate bias by comparing the model’s token probability of the masked words. 1,2,3

Model’s robustness can be enhanced by utilizing counterfactual examples11,12,8,13

For example, using sentiment to capture overall sentence polarity, regard ratio4,5 to measure language polarity and social perceptions of a demographic, offensive6, and toxicity5,7 as classifiers

Fine tuning models with controllable prefixes14

Hand-crafted prompts to mitigate bias in machine translation15

Generate prompts to equalize gender and race disparity in the dialogue generation task16

few-shot learning with proposed data interventions to mitigate bias in model17

1. Zhao et al., 2017; 2. Kurita et al., 2019; 3. Bordia and Bowman, 2019; 4. Sheng et al., 2019, 2020; 5. Dhamala et al., 2021; 6. Liu et al., 2020; 7. Perez et al., 2022; 8. Liu et al., 2020; 9. Zhang et al., 2018; 10. May et al., 2019; 11. Lu et al., 2019; 12. Maudslay et al., 2019; 13. Zmigrod et al., 2019; 14. Dinan et al.,2020; 15. Li and Liang, 2021; 16. Sheng et al.,2020; 17. Thakur et al.,2023;

__What is NEW in this paper?__

__Bias Mitigation __

__Bias Investigation __

Introduces a novel way to automatically synthesize test cases to measure global biases by leveraging  _reinforcement learning_ .

With  _disparity as reward functions_ , this method could more efficiently address potential bias in LLMs.

Proposes a  _gradient-free method _ which can mitigate LLM API’s biases without accessing and updating their parameters. Extends the context in ICL toward bias mitigation by utilizing and transforming bias examples into good demonstrations to mitigate bias

_Summarize contributions _

Proposed method utilizes RL to generate lots of difficult test cases that can effectively provoke bias in popular LLMs, such as ChatGPT, GPT-4, and Alpaca.

Proposes a simple but effective method to mitigate the bias found by these test cases without LLM parameter fine-tuning. Our proposal incorporates harmful test cases we found as examples and utilizes ICL to reduce bias in LLMs

---

In-context learning (ICL) (Dong et al., 2022) serves as another paradigm for LLMs to perform NLP tasks, where LLMs make predictions or responses only based on contexts augmented with a few demonstrations. One of the trending techniques based on ICL is Chain of Thought (CoT) (Wei et al., 2023; Kojima et al., 2022), which can let LLMs perform a series of intermediate reasoning steps and significantly improves the ability of large language models to perform complex reasoning.

Framework for automatically generating test cases and using them to mitigate bias

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/14.png" width="100%" height="100%">

---

In this work, they develop a framework that first generates high-quality test cases that may lead to biased responses in LLMs, as shown in the upper part of Figure 2. Then, they provide a strategy to mitigate these biases, as shown in the lower part of Figure 2.

_Bias Provocation_

Similar sentiments given these two inputs respectively

non-biased LLMs

The sentiment of y can be determined by using an off the-shelf sentiment classifier S.

the absolute difference as the metric for quantifying bias. For notation simplicity,                        has been denoted  as r(x)

A larger difference in r(x)  the test case x is more likely to elicit biased responses from LLMs.

Consequently, πg acquires the capability to generate text case sentences x associated with high r(x) values, effectively highlighting significant biases

1. Lu et al., 2019; 2. Maudslay et al., 2019; 3. Liu et al., 2019; 4. Zmigrod et al., 2019

Employed the concept of ICL with the generated ’demonstrations’ to show LLM how to respond to those tricky test cases in an unbiased way

---

In-context learning (ICL) (Dong et al., 2022) serves as another paradigm for LLMs to perform NLP tasks, where LLMs make predictions or responses only based on contexts augmented with a few demonstrations. One of the trending techniques based on ICL is Chain of Thought (CoT) (Wei et al., 2023; Kojima et al., 2022), which can let LLMs perform a series of intermediate reasoning steps and significantly improves the ability of large language models to perform complex reasoning.


 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/15.png" width="100%" height="100%">

The reward designed for a test case x is:

Maximizing the combined objective function in RL training:

1. Ouyang et al., 2022; 2. Schulman et al., 2017

__Bias Provocation & Mitigation Experiments__

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/16.png" width="100%" height="100%">

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/17.png" width="100%" height="100%">

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/18.png" width="100%" height="100%">

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/19.png" width="100%" height="100%">

Ablation study based on different numbers of demonstrations during mitigation.

---

Table-1: P-Chat and FT-Gen share a similar sentiment gap. After applying RL to provoke bias, each of the three target LLMs has a larger sentiment gap
Table-2: In the After RL section, there is a marginal increase in PPL scores, signifying a minor drop in the quality of sentences by post-RL generators. However, it’s a negligible increase, indicating that our produced test cases continue to be of high quality. Also, negligible change in the Self-BLEU scores of each LLM further implies the sustained diversity in our test cases. In summary, Table 2 shows the effectiveness of the RL method in preserving the generator’s ability to produce varied and top-quality test cases
To see how the number of demonstrations affects the performance of mitigation, we also do an ablation study based on various numbers of demonstrations, ranging from one to five. If we use more demonstration with the Top 5 strategy, we can mitigate bias in ChatGPT and GPT-4 better and are all better than those using the Sample 5 strategy. As for Alpaca, it can get the best result when using five demonstrations for both strategies

__Test cases and LLMs Responses Analysis__

The test cases for each of the three target LLMs exhibit a tendency to ask questions, but the nature of the questions differs

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/20.png" width="100%" height="100%">

Preference ratio of gender in responses for each LLM. Same means VADER gives the same scores to the two responses

---

VADER Sentiment Classifier (Hutto and Gilbert, 2014) as our metric for measuring sentiment scores in the responses of target LLMs. We chose the VADER sentiment analyzer since it is a rule-based sentiment analyzer that can significantly reduce training time in RL training.

__Demonstration of test cases for each target LLMs__

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/21.png" width="100%" height="100%">

__Limitations & Future work__

Self-defense in ChatGPT and GPT4

Demographic Categorization

Grammar and Semantic in Test Cases

ChatGPT and GPT4 are trained with safety concerns and have randomness in text generation, the test cases here found may not lead to responses with higher sentiment gaps every time when inference

Categorizing gender as either male or female. Nevertheless, this classification may create a division among individuals and may not be comprehensible to all

While generating test cases that maintain diversity to some extent, there may be some grammar or semantic mistakes in test cases. This might cause due to two reasons. The first is the degradation of GPT-2 medium. Or secondly, the naive implementation of CDA1 in the training loop

Future work should involve exploring methods to identify stronger and more robust test cases

In the future work gender neutral language are reasonable expectations that is inclusive to gender diverse people.

Future work should include using a larger test case generator like (Perez et al., 2022) and improving the perturbation method can also be the future works.

