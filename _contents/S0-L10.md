---
layout: post
title: FM fairness / bias issues 
lecture: S0-Intro
lectureVersion: next
extraContent: 
notes: team-2
video: team-5
tags:
- 1Basic
---

In this session, our readings cover: 

## Required Readings: 

### Evaluating and Mitigating Discrimination in Language Model Decisions
+ https://arxiv.org/abs/2312.03689
+ As language models (LMs) advance, interest is growing in applying them to high-stakes societal decisions, such as determining financing or housing eligibility. However, their potential for discrimination in such contexts raises ethical concerns, motivating the need for better methods to evaluate these risks. We present a method for proactively evaluating the potential discriminatory impact of LMs in a wide range of use cases, including hypothetical use cases where they have not yet been deployed. Specifically, we use an LM to generate a wide array of potential prompts that decision-makers may input into an LM, spanning 70 diverse decision scenarios across society, and systematically vary the demographic information in each prompt. Applying this methodology reveals patterns of both positive and negative discrimination in the Claude 2.0 model in select settings when no interventions are applied. While we do not endorse or permit the use of language models to make automated decisions for the high-risk use cases we study, we demonstrate techniques to significantly decrease both positive and negative discrimination through careful prompt engineering, providing pathways toward safer deployment in use cases where they may be appropriate. Our work enables developers and policymakers to anticipate, measure, and address discrimination as language model capabilities and applications continue to expand. We release our dataset and prompts at this https URL


## More Readings: 

### Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models
 + https://arxiv.org/abs/2310.11079

### Machine Learning in development: Let's talk about bias! 
  + https://huggingface.co/blog/ethics-soc-2 
  + https://huggingface.co/blog/evaluating-llm-bias 

### Exploring Social Bias in Chatbots using Stereotype Knowledge WNLP@ACL2019

### Bias and Fairness in Large Language Models: A Survey
  + https://arxiv.org/abs/2309.00770
  + Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly-available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.

### A Survey on Fairness in Large Language Models
  + https://arxiv.org/abs/2308.10149
  + Large language models (LLMs) have shown powerful performance and development prospect and are widely deployed in the real world. However, LLMs can capture social biases from unprocessed training data and propagate the biases to downstream tasks. Unfair LLM systems have undesirable social impacts and potential harms. In this paper, we provide a comprehensive review of related research on fairness in LLMs. First, for medium-scale LLMs, we introduce evaluation metrics and debiasing methods from the perspectives of intrinsic bias and extrinsic bias, respectively. Then, for large-scale LLMs, we introduce recent fairness research, including fairness evaluation, reasons for bias, and debiasing methods. Finally, we discuss and provide insight on the challenges and future directions for the development of fairness in LLMs.

 
## Prompt Designing: Mitigation Techniques

__Prompt interventions__

Mitigation in decision-making step

  <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/1.png" width="100%" height="100%">

---

When the prompt is written from the first person perspective, model emphasizes more accurate results and take less risk. Biases are injected through data. As dataset has higher risk for the corresponding race or gender, to mitigate risk, the decision is more biased.
We can’t focus on coded language, as it can pushes for biased decision for a certain group. 

__Prompt interventions__

Mitigation in decision-making step

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/2.png" width="100%" height="100%">

---

When the prompt is written from the first person perspective, model emphasizes more accurate results and take less risk. Biases are injected through data. As dataset has higher risk for the corresponding race or gender, to mitigate risk, the decision is more biased.
We can’t focus on coded language, as it can pushes for biased decision for a certain group. 

__Prompt interventions__

Mitigation in decision-making step

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/3.png" width="100%" height="100%">

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/4.png" width="100%" height="100%">

---

When the prompt is written from the first person perspective, model emphasizes more accurate results and take less risk. Biases are injected through data. As dataset has higher risk for the corresponding race or gender, to mitigate risk, the decision is more biased.
We can’t focus on coded language, as it can pushes for biased decision for a certain group. 

Prompt interventions

__Mitigation in decision-making step__

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/5.png" width="100%" height="100%">

---

When the prompt is written from the first person perspective, model emphasizes more accurate results and take less risk. Biases are injected through data. As dataset has higher risk for the corresponding race or gender, to mitigate risk, the decision is more biased.
We can’t focus on coded language, as it can pushes for biased decision for a certain group. 

Prompt interventions

__Mitigation in decision-making step__

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/6.png" width="100%" height="100%">

---

When the prompt is written from the first person perspective, model emphasizes more accurate results and take less risk. Biases are injected through data. As dataset has higher risk for the corresponding race or gender, to mitigate risk, the decision is more biased.
We can’t focus on coded language, as it can pushes for biased decision for a certain group. 

## Prompt intervention mitigates discrimination!

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/7.png" width="100%" height="100%">

---

Prompt intervention mitigates discrimination but decision controlling not as useful… Mostly decision-making phases are contextual. Biases is not defined explicitly. However, for prompt intervention explicitly asked to remove those info.

<span style="color:#FF0000">Not significant improvement</span>

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/8.png" width="100%" height="100%">
---

Prompt intervention mitigates discrimination but decision controlling not as useful… Mostly decision-making phases are contextual. Biases is not defined explicitly. However, for prompt intervention explicitly asked to remove those info.

<span style="color:#00B050">Noticeable improvement</span>

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/9.png" width="100%" height="100%">

---

Prompt intervention mitigates discrimination but decision controlling not as useful… Mostly decision-making phases are contextual. Biases is not defined explicitly. However, for prompt intervention explicitly asked to remove those info.

## Does the intervention distort the model decision?

<span style="color:#FF0000">Does it make decision of the model less useful?</span>

Ask for decision

Find correlation

<span style="color:#FF0000">Ask for decision</span>

Decision with intervention

---

Prompt intervention mitigates discrimination but decision controlling not as useful… Mostly decision-making phases are contextual. Biases is not defined explicitly. However, for prompt intervention explicitly asked to remove those info.

## Intervention maintains a high correlation with the original decision

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/10.png" width="100%" height="100%">

---

Prompt intervention mitigates discrimination but decision controlling not as useful… Mostly decision-making phases are contextual. Biases is not defined explicitly. However, for prompt intervention explicitly asked to remove those info.

## Evaluation Limitation

* Specific set of prompts
  * People may use wide variety of prompts
  * Do not use wide range of characteristics
    * Veteran status, income, health status
    * Selection of names
    * Consider only LLM’s decision
    * Do not consider intersectional effects
      * Race and age
      * Sensitivity study should be on larger scale

---

Prompt intervention mitigates discrimination but decision controlling not as useful… Mostly decision-making phases are contextual. Biases is not defined explicitly. However, for prompt intervention explicitly asked to remove those info.



## Presentation Outline

Evaluating and Mitigating Discrimination in Language Model Decisions

Bias and Fairness Evaluation

<span style="color:#00B050">Gender Bias in LLM</span>

__Gender Bias Provocation and Mitigation in LLMs__

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/12.png" width="100%" height="100%">

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/13.png" width="100%" height="100%">

_Traditional biases investigation methods:_  Rely on human-written test cases

<span style="color:#C00000">Expensive</span>

<span style="color:#C00000">Limited</span>

Example of test cases and responses of  _Alpaca_  before and after mitigation

_Introducing a new mitigation strategy:_

Automatically generates test cases to detect LLMs’ potential gender bias.

__Related Previous Work__

_Bias Mitigation in Natural Language Generation_

_Bias Investigation in Natural Language Generation_

Local bias-based

Global bias-based

Mainly aim to reduce bias by replacing or deleting biased words in training data

Hand-crafted templates to evaluate bias

Adversarial Learning8,9 which fine-tunes the model using an adversarial loss to eliminate bias.

Concept of Null space projection10 to eliminate gender features in models

Multiple classifiers to evaluate bias by comparing the classification results of generated texts from various perspectives

Counterfactual Data Augmentation (CDA)

For example, the template can be a sentence with some masked words. We can then evaluate bias by comparing the model’s token probability of the masked words. 1,2,3

Model’s robustness can be enhanced by utilizing counterfactual examples11,12,8,13

For example, using sentiment to capture overall sentence polarity, regard ratio4,5 to measure language polarity and social perceptions of a demographic, offensive6, and toxicity5,7 as classifiers

Fine tuning models with controllable prefixes14

Hand-crafted prompts to mitigate bias in machine translation15

Generate prompts to equalize gender and race disparity in the dialogue generation task16

few-shot learning with proposed data interventions to mitigate bias in model17

1. Zhao et al., 2017; 2. Kurita et al., 2019; 3. Bordia and Bowman, 2019; 4. Sheng et al., 2019, 2020; 5. Dhamala et al., 2021; 6. Liu et al., 2020; 7. Perez et al., 2022; 8. Liu et al., 2020; 9. Zhang et al., 2018; 10. May et al., 2019; 11. Lu et al., 2019; 12. Maudslay et al., 2019; 13. Zmigrod et al., 2019; 14. Dinan et al.,2020; 15. Li and Liang, 2021; 16. Sheng et al.,2020; 17. Thakur et al.,2023;

__What is NEW in this paper?__

__Bias Mitigation __

__Bias Investigation __

Introduces a novel way to automatically synthesize test cases to measure global biases by leveraging  _reinforcement learning_ .

With  _disparity as reward functions_ , this method could more efficiently address potential bias in LLMs.

Proposes a  _gradient-free method _ which can mitigate LLM API’s biases without accessing and updating their parameters. Extends the context in ICL toward bias mitigation by utilizing and transforming bias examples into good demonstrations to mitigate bias

_Summarize contributions _

Proposed method utilizes RL to generate lots of difficult test cases that can effectively provoke bias in popular LLMs, such as ChatGPT, GPT-4, and Alpaca.

Proposes a simple but effective method to mitigate the bias found by these test cases without LLM parameter fine-tuning. Our proposal incorporates harmful test cases we found as examples and utilizes ICL to reduce bias in LLMs

---

In-context learning (ICL) (Dong et al., 2022) serves as another paradigm for LLMs to perform NLP tasks, where LLMs make predictions or responses only based on contexts augmented with a few demonstrations. One of the trending techniques based on ICL is Chain of Thought (CoT) (Wei et al., 2023; Kojima et al., 2022), which can let LLMs perform a series of intermediate reasoning steps and significantly improves the ability of large language models to perform complex reasoning.

Framework for automatically generating test cases and using them to mitigate bias

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/14.png" width="100%" height="100%">

---

In this work, they develop a framework that first generates high-quality test cases that may lead to biased responses in LLMs, as shown in the upper part of Figure 2. Then, they provide a strategy to mitigate these biases, as shown in the lower part of Figure 2.

_Bias Provocation_

Similar sentiments given these two inputs respectively

non-biased LLMs

The sentiment of y can be determined by using an off the-shelf sentiment classifier S.

the absolute difference as the metric for quantifying bias. For notation simplicity,                        has been denoted  as r(x)

A larger difference in r(x)  the test case x is more likely to elicit biased responses from LLMs.

Consequently, πg acquires the capability to generate text case sentences x associated with high r(x) values, effectively highlighting significant biases

1. Lu et al., 2019; 2. Maudslay et al., 2019; 3. Liu et al., 2019; 4. Zmigrod et al., 2019

Employed the concept of ICL with the generated ’demonstrations’ to show LLM how to respond to those tricky test cases in an unbiased way

---

In-context learning (ICL) (Dong et al., 2022) serves as another paradigm for LLMs to perform NLP tasks, where LLMs make predictions or responses only based on contexts augmented with a few demonstrations. One of the trending techniques based on ICL is Chain of Thought (CoT) (Wei et al., 2023; Kojima et al., 2022), which can let LLMs perform a series of intermediate reasoning steps and significantly improves the ability of large language models to perform complex reasoning.


 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/15.png" width="100%" height="100%">

The reward designed for a test case x is:

Maximizing the combined objective function in RL training:

1. Ouyang et al., 2022; 2. Schulman et al., 2017

__Bias Provocation & Mitigation Experiments__

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/16.png" width="100%" height="100%">

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/17.png" width="100%" height="100%">

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/18.png" width="100%" height="100%">

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/19.png" width="100%" height="100%">

Ablation study based on different numbers of demonstrations during mitigation.

---

Table-1: P-Chat and FT-Gen share a similar sentiment gap. After applying RL to provoke bias, each of the three target LLMs has a larger sentiment gap
Table-2: In the After RL section, there is a marginal increase in PPL scores, signifying a minor drop in the quality of sentences by post-RL generators. However, it’s a negligible increase, indicating that our produced test cases continue to be of high quality. Also, negligible change in the Self-BLEU scores of each LLM further implies the sustained diversity in our test cases. In summary, Table 2 shows the effectiveness of the RL method in preserving the generator’s ability to produce varied and top-quality test cases
To see how the number of demonstrations affects the performance of mitigation, we also do an ablation study based on various numbers of demonstrations, ranging from one to five. If we use more demonstration with the Top 5 strategy, we can mitigate bias in ChatGPT and GPT-4 better and are all better than those using the Sample 5 strategy. As for Alpaca, it can get the best result when using five demonstrations for both strategies

__Test cases and LLMs Responses Analysis__

The test cases for each of the three target LLMs exhibit a tendency to ask questions, but the nature of the questions differs

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/20.png" width="100%" height="100%">

Preference ratio of gender in responses for each LLM. Same means VADER gives the same scores to the two responses

---

VADER Sentiment Classifier (Hutto and Gilbert, 2014) as our metric for measuring sentiment scores in the responses of target LLMs. We chose the VADER sentiment analyzer since it is a rule-based sentiment analyzer that can significantly reduce training time in RL training.

__Demonstration of test cases for each target LLMs__

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/21.png" width="100%" height="100%">

__Limitations & Future work__

Self-defense in ChatGPT and GPT4

Demographic Categorization

Grammar and Semantic in Test Cases

ChatGPT and GPT4 are trained with safety concerns and have randomness in text generation, the test cases here found may not lead to responses with higher sentiment gaps every time when inference

Categorizing gender as either male or female. Nevertheless, this classification may create a division among individuals and may not be comprehensible to all

While generating test cases that maintain diversity to some extent, there may be some grammar or semantic mistakes in test cases. This might cause due to two reasons. The first is the degradation of GPT-2 medium. Or secondly, the naive implementation of CDA1 in the training loop

Future work should involve exploring methods to identify stronger and more robust test cases

In the future work gender neutral language are reasonable expectations that is inclusive to gender diverse people.

Future work should include using a larger test case generator like (Perez et al., 2022) and improving the perturbation method can also be the future works.

