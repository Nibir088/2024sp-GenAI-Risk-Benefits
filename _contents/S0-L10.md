---
layout: post
title: FM fairness / bias issues 
lecture: S0-Intro
lectureVersion: next
extraContent: 
notes: team-2
video: team-5
tags:
- 1Basic
---

In this session, our readings cover: 

## Required Readings: 

### Evaluating and Mitigating Discrimination in Language Model Decisions
+ https://arxiv.org/abs/2312.03689
+ As language models (LMs) advance, interest is growing in applying them to high-stakes societal decisions, such as determining financing or housing eligibility. However, their potential for discrimination in such contexts raises ethical concerns, motivating the need for better methods to evaluate these risks. We present a method for proactively evaluating the potential discriminatory impact of LMs in a wide range of use cases, including hypothetical use cases where they have not yet been deployed. Specifically, we use an LM to generate a wide array of potential prompts that decision-makers may input into an LM, spanning 70 diverse decision scenarios across society, and systematically vary the demographic information in each prompt. Applying this methodology reveals patterns of both positive and negative discrimination in the Claude 2.0 model in select settings when no interventions are applied. While we do not endorse or permit the use of language models to make automated decisions for the high-risk use cases we study, we demonstrate techniques to significantly decrease both positive and negative discrimination through careful prompt engineering, providing pathways toward safer deployment in use cases where they may be appropriate. Our work enables developers and policymakers to anticipate, measure, and address discrimination as language model capabilities and applications continue to expand. We release our dataset and prompts at this https URL


## More Readings: 

### Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models
 + https://arxiv.org/abs/2310.11079

### Machine Learning in development: Let's talk about bias! 
  + https://huggingface.co/blog/ethics-soc-2 
  + https://huggingface.co/blog/evaluating-llm-bias 

### Exploring Social Bias in Chatbots using Stereotype Knowledge WNLP@ACL2019

### Bias and Fairness in Large Language Models: A Survey
  + https://arxiv.org/abs/2309.00770
  + Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly-available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.

### A Survey on Fairness in Large Language Models
  + https://arxiv.org/abs/2308.10149
  + Large language models (LLMs) have shown powerful performance and development prospect and are widely deployed in the real world. However, LLMs can capture social biases from unprocessed training data and propagate the biases to downstream tasks. Unfair LLM systems have undesirable social impacts and potential harms. In this paper, we provide a comprehensive review of related research on fairness in LLMs. First, for medium-scale LLMs, we introduce evaluation metrics and debiasing methods from the perspectives of intrinsic bias and extrinsic bias, respectively. Then, for large-scale LLMs, we introduce recent fairness research, including fairness evaluation, reasons for bias, and debiasing methods. Finally, we discuss and provide insight on the challenges and future directions for the development of fairness in LLMs.
  
# In this session, our blog covers: 
##  Bias and Fairness in Large Language Model

### 1 &nbsp; &nbsp; Formal Definition of Bias and Fairness (LLM context)
#### 1.1 &nbsp; Preliminaries
+ __Definition 1: Large Language Model__
    + A large language model (LLM) M parameterized by θ is a Transformer-based model with an autoregressive, autoencoding, or encoder-decoder architecture that has been trained on a large corpus of hundreds of millions to trillions of tokens. LLMs encompass pre-trained models.
+ __Definition 2: Evaluation Metric__
    + For some evaluation dataset (D) there exists a subset of metrics ψ(D) (from space of all metrics Ψ) that are appropriate for D 
<img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/screenshot1.jpg" width="100%" height="100%">

#### 1.2 &nbsp; Social Bias and Fairness
+ __Definition 3: Social Group__
    + A social group G ∈ G is a subset of the population that shares an identity trait, which may be fixed, contextual, or socially constructed. Examples include groups legally protected by anti-discrimination law (i.e., "protected groups" or "protected classes" under federal United States law), including age, color, disability, gender identity, national origin, race, religion, sex, and sexual orientation.
    + Caution: social groups are often socially constructed. So, they can change overtime. Harms experienced by each group vary greatly due to historical, structural injustice. 
+ __Definition 4: Protected Attribute__
    + A protected attribute is the shared identity trait that determines the group identity of a social group.
+ __Definition 5: Group Fairness__
+ <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/screenshot2.jpg" width="100%" height="100%">
+ __Definition 6: Individual Fairness__
<img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/screenshot3.jpg" width="100%" height="100%">
+ __Definition 7: Social Bias__
    + Social bias broadly encompasses disparate treatment or outcomes between social groups that arise from historical and structural power asymmetries.
<img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/table1.jpg" width="100%" height="100%">

#### 1.3 &nbsp; Bias in NLP Tasks
+ __Text Generation__
    + Predicting next token: "The man was known for [BLANK]" vs. "The woman was known for [BLANK]"
+ __Machine Translation__
    + Translation defaults to masculine words: “I am happy” is translated into "je suis heureux" masculine more often as opposed to the feminine form "je suis heureuse." 
+ __Information Retrieval__
    + Retrieved documents have more masculine-related concepts instead of feminine.
+ __Question-Answering__
    + Model relies on stereotypes to answer questions.
    + e.g. racial bias in answering question about drugs
+ __NL Inference__
    + Predicting a premise: whether a hypothesis entails or contradicts.
    + Make invalid inference.
    + e.g. "the accountant ate a bagel" (ACTUAL) vs "the man ate a bagel" or "the woman ate a bagel" (WRONG)
+ __Classification__
    + Toxicity Models misclassify African American tweets as negative more often then in Standard American English

#### 1.4 &nbsp; Fairness Constraints
<img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/screenshot4.jpg" width="100%" height="100%">

### 2 &nbsp; &nbsp; Taxonomy of Metrics used to evaluate Bias 
#### 2.1 &nbsp; Facets of Metrics
+ __Task Specific__
    + Different NLP task types (text generation, classification etc.) need different metrics.
+ __Bias Type__
    + Bias type varies between datasets so metrics might change.
+ __Data structure (input to model)__
    + e.g.: dataset consists of single pairs of sentences, one more biased than the other, this will alter our metric needs.
+ __Data Structure (output from model)__
    + Output type can change metric. 
    + Output could be embeddings, the estimated probabilities from the model, or the generated text from the model.

#### 2.2 &nbsp; Taxonomy of Metrics based on What They Use1. 
+ __Embedding-based Metrics__
    + Using the dense vector representations to measure bias, which are typically contextual sentence embeddings.
+ __Probability-based Metrics__
    + Using the model-assigned probabilities to estimate bias (e.g., to score text pairs or answer multiple-choice questions).
+ __Generated text-based Metrics__
    + Using the model-generated text conditioned on a prompt (e.g., to measure co-occurrence patterns or compare outputs generated from perturbed prompts).

#### 2.3 &nbsp; Embedding-based Metrics
+ __Word Embedding Metrics__
    + After encoder has generated vectors from words, we see how bias can shift certain words closer to others
    <p align="center">
        <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/picture1.jpg" width="40%" height="40%">
    </p>
    + __WEAT (pre-LLM NLP era)__ measures associations between social group concepts (e.g., masculine and feminine words) and neutral attributes (e.g., family and occupation words). 
        + For protected attributes A1, A2 and neutral words W1 and W2. We define test statistic f:
    <p align="center">
        <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/screenshot5.jpg" width="70%" height="70%">
    </p>
+ __Sentence Embedding Metrics__
    + Instead of using static word embeddings, LLMs use embeddings learned in the context of a sentence, and are more appropriately paired with embedding metrics for sentence-level encoders. Using full sentences also enables more targeted evaluation of various dimensions of bias, using sentence templates that probe for specific stereotypical associations.
    + __SEAT (Sentence edition of WEAT)__ compares sets of sentences, rather than sets of words, by applying WEAT to the vector representation of a sentence. 
<img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/screenshot6.jpg" width="100%" height="100%">
+ __Problems of Embedding-based metrics__
    + Several works point out that biases in the embedding space have only weak or inconsistent relationships with biases in downstream tasks (Cabello et al., 2023; Cao et al., 2022; Goldfarb-Tarrant et al., 2021; Orgad & Belinkov, 2022; Orgad et al., 2022; Steed et al., 2022).
    + Goldfarb-Tarrant et al. (2021) find no reliable correlation at all, and Cabello et al. (2023) illustrate that associations between the representations of protected attribute and other words can be independent of downstream performance disparities, if certain assumptions of social groups’ language use are violated
    + These works demonstrate that bias in representations and bias in downstream applications should not be conflated, which may limit the value of embedding-based metrics

#### 2.4 &nbsp; Probability-based Metrics
+ The probability of a token can be derived by masking a word in a sentence and asking a masked language model to fill in the blank.
+ __Masked Token Methods__
    <p align="center">
        <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/picture2.jpg" width="50%" height="50%">
    </p>
+ __Pseudo-Log-Likelihood Methods__
<img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/screenshot7.jpg" width="100%" height="100%">
    + PLL should be same for both cases for it to be unbiased.
+ __Problems of Probability-Based Metrics__
    + Probability-based metrics may be only weakly correlated with biases that appear in downstream tasks.
    + Masked token metrics rely on templates, which often lack semantic and syntactic diversity and have highly limited sets of target words to instantiate the template, which can cause the metrics to lack generalizability and reliability.
    + Nearly all metrics assume binary social groups or binary pairs, which may fail to account for more complex groupings or relationships.

#### 2.5 &nbsp; Generated Text-Based Metrics 
+ __Distribution Metrics__
    + __Co-Occurrence Bias Score__ measures the co-occurrence of tokens with gendered words in a corpus of generated text.
    <p align="center">
        <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/screenshot8.jpg" width="50%" height="50%">
    </p>
    + __Demographic Representation (DR)__ compares the frequency of mentions of social groups to the original data distribution.
    <p align="center">
        <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/screenshot9.jpg" width="55%" height="55%">
    </p>
+ __Classifier Metrics__
    + rely on an auxiliary model to score generated text outputs for their toxicity, sentiment, or any other dimension of bias. Bias can be detected if text generated from similar prompts, but with different social groups, are classified differently.
    + __Expected Maximum Toxicity (EMT)__ computed over 25 generations and use mean and standard deviation. This metric characterizes the worst-case generations of an LLM. For a toxicity detector c : Y → [0, 1]
    <p align="center">
        <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/picture3.jpg" width="60%" height="60%">
    </p>
    + __Toxicity Probability (TP)__ measures the empirical probability of generating at least one text with a toxicity score ≥ 0.5 over the 25 generations, which captures how frequently the LLM generates text that is toxic.
    <p align="center">
        <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/picture4.jpg" width="40%" height="40%">
    </p>
+ __Lexicon Metrics__
    + Lexicon-based metrics perform a word-level analysis of the generated output, comparing each word to a pre-compiled list of harmful words, or assigning each word a pre-computed bias score
    + __HONEST__ measures the number of hurtful completions. 
<img src="{{ site.baseurl }}/Lectures/S0-L10/images/p1/screenshot10.jpg" width="110%" height="110%">

+ __Problems of Generated Text-Based Metrics__
    + Decoding parameters, including the number of tokens generated, the temperature for sampling, and the top-k choice for beam search, can drastically change the level of bias, which can lead to contradicting results for the same metric with the same evaluation datasets, but different parameter choices.
    + Classifier-based metrics may be unreliable if the classifier itself has its own biases. (Toxicity classifier biased to flagging African American English more)
    + Lexicon-based metrics may be overly coarse and overlook relational patterns between words, sentences, or phrases. 

 

### 5 &nbsp; Prompt Designing: Mitigation Techniques
### 5.1. &nbsp; Appending statements to prompts
We append various statements to the end of prompts:
+ Statements saying demographics should not influence
the decision, with 1x, 2x, and 4x repetitions of
the word “really” in “really important.” (Really
(1x) don’t discriminate, Really
(2x) don’t discriminate, Really (4x)
don’t discriminate)
+ A statement that affirmative action should not affect the
decision. (Don’t use affirmative action)
+ Statements that any provided demographic information
was a technical quirk (Ignore demographics)
that protected characteristics cannot legally be considered (Illegal to discriminate) and a combination of both (Illegal + Ignore).
---

  <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/1.png" width="100%" height="100%">

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/2.png" width="100%" height="100%">

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/3.png" width="100%" height="100%">

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/4.png" width="100%" height="100%">

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/5.png" width="100%" height="100%">

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/6.png" width="100%" height="100%">

When the prompt is written from the first person perspective, model emphasizes more accurate results and take less risk. Biases are injected through data. As dataset has higher risk for the corresponding race or gender, to mitigate risk, the decision is more biased.
We can’t focus on coded language, as it can pushes for biased decision for a certain group. 

---

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/7.png" width="100%" height="100%">

### 5.3. &nbsp; Results
As shown in Figure 5, several of the interventions
we explore are quite effective, especially Illegal
to discriminate, Ignore demographics,
Illegal + Ignore. Many of these interventions
significantly reduce the discrimination score, often
approaching 0. Other interventions appear to reduce the
discrimination score by a more moderate amount.
These results demonstrate that positive and negative discrimination on the questions we consider can be significantly
reduced, and in some cases removed altogether, by a set of
prompt-based interventions.

---



### 5.4. &nbsp; Do the interventions distort the model’s decisions?
While the success of these interventions at reducing positive
and negative discrimination is notable, an important remaining question is whether they make the decisions of the model
less useful. For example, a simple way to reduce discrimination is to output the exact same prediction for every input. In
this work, we study hypothetical decision questions that are
subjective, and do not have ground-truth answers. However,
we can still measure how much the responses of the model
change when an intervention is applied.

Concretely, we compute the Pearson correlation coefficient
between the decisions before and after the intervention
is applied. In Figure 6, we show a scatter plot comparing this correlation coefficient and the average discrimination across demographic groups (age, Black, Asian, Hispanic, Native American, female, and non-binary). We see
that a wide range of interventions produce small amounts
of discrimination while maintaining very high correlation
with the original decisions. Notably, the Illegal to
discriminate and Ignore demographics interventions (Prompt 2) appear to achieve a good tradeoff between low discrimination score (≈ 0.15) and high correlation with the original decisions (≈ 92%).


### 6. &nbsp; Discussion

Prompt intervention mitigates discrimination but decision controlling not as useful Mostly decision-making phases are contextual. Biases is not defined explicitly. However, for prompt intervention explicitly asked to remove those info.

Intervention maintains a high correlation with the original decision

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/10.png" width="100%" height="100%">



### 6.1 &nbsp; Limitations 
+ Limited Input Formats: It only evaluated the model on paragraphs, not real-world formats like resumes or dialogues.
+ Limited Scope of Characteristics: It only considered race, gender, and age, not other important characteristics like income or religion.
+ Potential Bias: Using a language model to generate evaluations might unintentionally limit the considered applications.
+ Challenges in Proxy Discrimination: Choosing names associated with different demographics is complex, and there might be other sources of bias to explore.
+ Focus on Model Decisions, not User Impact: It only analyzes the model's decisions, not how they influence users in real-world settings.
+ Limited Analysis of Intersectionality: It only examines individual characteristics, not how they interact (e.g., race and gender combined).
+ Sensitivity to Prompts: Slight variations in how prompts are phrased can affect the model's behavior, potentially impacting the study's conclusions.

### 6.2 &nbsp; Should models be used for the applications we study?
 
+ Limited Scope: The presented evaluation methods don't guarantee a model's suitability for real-world scenarios with high consequences.
+ Complex Interactions: The way models interact with people and existing biases (like automation bias) necessitates a broader "sociotechnical" approach including policies and regulations.
+ Beyond Fairness: While discriminatory impacts are critical, ensuring the model actually performs its intended task effectively is equally important.
+ Shared Responsibility: Ultimately, the decision of deploying such models for high-stakes situations should involve broader societal dialogue and existing legal frameworks, not solely individual entities.
It is ultimately argued for a cautious and collective approach to using language models in critical decision-making, considering both ethical and practical aspects. 


### 6.3 &nbsp;  How should positive discrimination be addressed?
The complex issue of positive discrimination identified by their research and recognizes the ongoing debates surrounding its correction. Instead of taking a stance on the ethical or legal aspects of positive discrimination (often discussed within the context of affirmative action), they focus on providing tools for various stakeholders. These tools:

+ Measure discrimination: Help stakeholders quantify potential biases in AI systems across different scenarios.
+ Control discrimination: Offer a "dial" through prompting techniques to mitigate the degree of positive discrimination in the model's outputs.


### 6.4 &nbsp;  Where does this behavior come from
+ Human bias in training data: The raters who provided feedback during training might hold different preferences from the general population, influencing the model's perception.
+ Overgeneralization in reinforcement learning: While the model might have been trained to avoid specific biases, it could have misinterpreted or overgeneralized this training, leading to a counter-intuitive favoring of the targeted groups.?



### 7 &nbsp;  Conclusions
In summary, this work draws on a rich foundation of techniques across machine learning and the social sciences to
proactively assess and mitigate the risk of language model
discrimination. 

---
<div style="text-align: center;">
<H1> Learning from Red Teaming &nbsp;: Gender Bias Provocation and Mitigation in Large Language Models </H1>
</div>

---


### 1 &nbsp; Gender Bias Provocation and Mitigation in LLM

This paper proposes a novel method to automatically detect and mitigate bias in large language models (LLMs) like ChatGPT and GPT-4.

#### Current methods:

+ Bias detection: Existing approaches rely on either crowdsourced data (expensive and unreliable) or hand-crafted templates (limited scope).
+ Bias mitigation: Past methods utilize algorithms or data, but often require significant human effort and struggle to quantify effectiveness.


 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/12.png" width="100%" height="100%">

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/13.png" width="100%" height="100%">


This work develops a system that uses reinforcement learning (RL) to generate diverse test cases specifically designed to expose bias in LLMs.
Moreover, the paper primarily focuses on detecting and mitigating gender bias. The example shows how different responses to sentences with swapped gender keywords indicate bias.
The proposed method uses in-context learning (ICL) to mitigate identified biases by providing the generated test cases as examples to the LLM, effectively retraining it without modifying core parameters (useful for online APIs).

#### Key contributions: 
+ Automatic and efficient bias detection: The method uses RL to generate large sets of effective test cases, uncovering potential biases in LLMs.
+ Bias mitigation without parameter adjustments: The proposed technique tackles bias using ICL and the generated test cases, avoiding the need for fine-tuning which may not be feasible in all scenarios.

### 2 &nbsp; Related Previous Work

#### Bias Mitigation in Natural Language Generation
Researchers are increasingly concerned about societal bias reflected in natural language generation (NLG) systems. To address this, various methods have been proposed to measure bias in these systems. Existing approaches fall into two main categories: local and global bias-based methods.

Local methods rely on hand-crafted templates with masked words. Researchers then evaluate bias by comparing the model's likelihood of different words filling these masks. For instance, they might compare the probability of "doctor" and "nurse" filling the mask in the sentence "The [masked word] is intelligent." For example, the template can be a sentence with some masked words. We can then evaluate bias by comparing the model’s token probability of the masked words.

Global methods, on the other hand, utilize multiple classifiers to analyze generated text from various perspectives. These classifiers can focus on different aspects, such as overall sentiment, how the text portrays specific demographics, or the presence of offensive language. For example, using sentiment to capture overall sentence polarity, regard ratio to measure language polarity and social perceptions of a demographic, offensive6, and toxicity as classifiers.

#### Bias Mitigation in Natural Language Generation
To reduce bias in natural language generation (NLG), researchers have adopted two main approaches: modifying the algorithms themselves (algorithm-based) and improving the training data (data-based).

Algorithm-based methods aim to adjust the NLG model internally. One technique, Adversarial Learning, trains the model alongside an "adversary" that exposes its biases, helping it learn to avoid biased outputs. Another approach, Null Space Projection, removes specific features (like gender) from the model's language representation, aiming to lessen bias based on those removed traits.

Data-based methods, on the other hand, focus on enhancing the training data used to train NLG models. One approach, Counterfactual Data Augmentation (CDA), creates new training examples addressing potential biases in the original data, making the model more robust against real-world biases. Other data-based methods include modifying training data with specific prefixes to guide the model or providing specific instructions (hand-crafted prompts) within the training data to encourage fairer outputs.



#### What is NEW in this paper?

##### Bias Mitigation
Proposes a gradient-free method which can mitigate LLM API’s biases without accessing and updating their parameters. Extends the context in ICL toward bias mitigation by utilizing and transforming bias examples into good demonstrations to mitigate bias
 


##### Bias Investigation
Introduces a novel way to automatically synthesize test cases to measure global biases by leveraging reinforcement learning. 
With disparity as reward functions, this method could more efficiently address potential bias in LLMs.

##### Summarized contributions :
+ Proposed method utilizes RL to generate lots of difficult test cases that can effectively provoke bias in popular LLMs, such as ChatGPT, GPT-4, and Alpaca.
 
+ Proposes a simple but effective method to mitigate the bias found by these test cases without LLM parameter fine-tuning. Our proposal incorporates harmful test cases we found as examples and utilizes ICL to reduce bias in LLMs


---
### 3. &nbsp; Methodology 
In-context learning (ICL) (Dong et al., 2022) serves as another paradigm for LLMs to perform NLP tasks, where LLMs make predictions or responses only based on contexts augmented with a few demonstrations. One of the trending techniques based on ICL is Chain of Thought (CoT) (Wei et al., 2023; Kojima et al., 2022), which can let LLMs perform a series of intermediate reasoning steps and significantly improves the ability of large language models to perform complex reasoning.

Framework for automatically generating test cases and using them to mitigate bias

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/14.png" width="100%" height="100%">

---

In this work, they develop a framework that first generates high-quality test cases that may lead to biased responses in LLMs, as shown in the upper part of Figure 2. Then, they provide a strategy to mitigate these biases, as shown in the lower part of Figure 2.

### 3.1. &nbsp; Bias Provocation

This paper defines bias in large language models (LLMs) as generating different sentiments for two sentences that differ only in gender-specific terms. They use a technique called Counterfactual Data Augmentation (CDA) to create these sentence pairs and then measure the sentiment difference using a pre-existing sentiment classifier. A larger difference indicates a stronger bias.

To efficiently find sentences that elicit biased responses (high sentiment difference), the paper proposes training a separate "generator" model using Reinforcement Learning (RL). This generator is rewarded for producing sentences that lead to high sentiment differences, essentially learning to identify and highlight potential biases in other LLMs. This framework is flexible and can be applied to different definitions of bias, not just gender bias.

### 3.2. &nbsp; Bias Mitigation

This paper tackles bias in large language models (LLMs) by first identifying it. They define bias as different sentiments generated for sentences differing only in gender. They use a "generator" model trained with Reinforcement Learning to find these biased cases.

Next, they aim to fix the bias using "in-context learning" (ICL). They create "demonstrations" by showing the LLM unbiased responses to previously identified biased cases. These demonstrations are then incorporated into the LLM's input, essentially training it to avoid similar biases in the future. This approach is advantageous as it avoids fine-tuning, making it adaptable to various situations.

### 4 &nbsp; Experimental Setup:
#### 4.1 &nbsp; RL Algorithm
Reinforcement Learning (RL) is used to train the generator model.
The model aims to maximize the expected bias it detects in other LLMs (represented by Ex∼πg [r(x)]).
The model is initialized from a pre-trained GPT-2 model and uses a specific RL algorithm called PPO-ptx.
A regularization term is added to the reward function to control the model's behavior and prevent it from getting stuck in a single mode.
The reward designed for a test case x is
 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/15.png" width="40%" height="20%">



Maximizing the combined objective function in RL training:
 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/22.png" width="60%" height="30%">


1. Ouyang et al., 2022; 2. Schulman et al., 2017

__Bias Provocation & Mitigation Experiments__

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/16.png" width="100%" height="100%">

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/17.png" width="100%" height="100%">

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/18.png" width="100%" height="100%">

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/19.png" width="100%" height="100%">

Ablation study based on different numbers of demonstrations during mitigation.

---

Table-1: P-Chat and FT-Gen share a similar sentiment gap. After applying RL to provoke bias, each of the three target LLMs has a larger sentiment gap
Table-2: In the After RL section, there is a marginal increase in PPL scores, signifying a minor drop in the quality of sentences by post-RL generators. However, it’s a negligible increase, indicating that our produced test cases continue to be of high quality. Also, negligible change in the Self-BLEU scores of each LLM further implies the sustained diversity in our test cases. In summary, Table 2 shows the effectiveness of the RL method in preserving the generator’s ability to produce varied and top-quality test cases
To see how the number of demonstrations affects the performance of mitigation, we also do an ablation study based on various numbers of demonstrations, ranging from one to five. If we use more demonstration with the Top 5 strategy, we can mitigate bias in ChatGPT and GPT-4 better and are all better than those using the Sample 5 strategy. As for Alpaca, it can get the best result when using five demonstrations for both strategies

__Test cases and LLMs Responses Analysis__

The test cases for each of the three target LLMs exhibit a tendency to ask questions, but the nature of the questions differs

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/20.png" width="100%" height="100%">

Preference ratio of gender in responses for each LLM. Same means VADER gives the same scores to the two responses

---

VADER Sentiment Classifier (Hutto and Gilbert, 2014) as our metric for measuring sentiment scores in the responses of target LLMs. We chose the VADER sentiment analyzer since it is a rule-based sentiment analyzer that can significantly reduce training time in RL training.

__Demonstration of test cases for each target LLMs__

 <img src="{{ site.baseurl }}/Lectures/S0-L10/images/p5/21.png" width="100%" height="100%">

__Limitations & Future work__

Self-defense in ChatGPT and GPT4

Demographic Categorization

Grammar and Semantic in Test Cases

ChatGPT and GPT4 are trained with safety concerns and have randomness in text generation, the test cases here found may not lead to responses with higher sentiment gaps every time when inference

Categorizing gender as either male or female. Nevertheless, this classification may create a division among individuals and may not be comprehensible to all

While generating test cases that maintain diversity to some extent, there may be some grammar or semantic mistakes in test cases. This might cause due to two reasons. The first is the degradation of GPT-2 medium. Or secondly, the naive implementation of CDA1 in the training loop

Future work should involve exploring methods to identify stronger and more robust test cases

In the future work gender neutral language are reasonable expectations that is inclusive to gender diverse people.

Future work should include using a larger test case generator like (Perez et al., 2022) and improving the perturbation method can also be the future works.

