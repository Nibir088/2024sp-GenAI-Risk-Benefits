---
layout: post
title: Open Source LLM - Mistral Data preparation 
lecture: 
lectureVersion: current
extraContent: 
notes: team-6
video: team-6
tags:
- BasicLLM
---

In this session, our readings cover: 

## Required Readings: 

### Mistral 7B 
  + https://mistral.ai/news/announcing-mistral-7b/
  + We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.


## More Readings: 

### OLMo: Accelerating the Science of Language Models
+ https://arxiv.org/abs/2402.00838

Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.

### Mixtral of Experts
  + https://arxiv.org/abs/2401.04088
  + We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.




### - Llama 2: Open Foundation and Fine-Tuned Chat Models
+ In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.

### The Pile: An 800GB Dataset of Diverse Text for Language Modeling
  + https://arxiv.org/abs/2101.00027
  + Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present \textit{the Pile}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.


# Section 1: The Pile

In this section, we are going to talk introduce The pile, an open source dataset for diverse text for language modeling.

### Motivation

Their work is driven by several key considerations. As the size of Large Language Models (LLMs) continues to expand rapidly, so does the need for vast amounts of data to effectively train these models. However, major players in the tech industry, such as Google and OpenAI, tend to keep their models and data closely guarded due to their commercial interests. Inspired by the principles of open-source software, they advocate for a similar ethos in the realm of LLMs. Open-sourcing data offers numerous advantages, including enhanced accessibility, opportunities for community collaboration, and the establishment of robust benchmarking and evaluation standards.


<img src="{{ site.baseurl }}/Lectures/S0-L06/images/piles/Motivation.jpg" width="100%" height="100%">

In line with this philosophy, various open-source datasets already exist on the internet, including The Common Crawl, RefinedWeb, Starcoder Data, and C4. However, in this section, they introduce a new and unique addition: The Pile. Their primary objective with The Pile is to enhance data diversity, thereby enriching the dataset's capabilities for modeling and training.

### The Pile Components 

<img src="{{ site.baseurl }}/Lectures/S0-L06/images/piles/Improvement.jpg" width="100%" height="100%">

The Pile comprises an 800GB dataset curated from 22 diverse datasets, covering a wide range of domains such as Academic, Internet, Prose, Dialogue, and Miscellaneous. The composition of The Pile by category is illustrated in Figure 1, with a more detailed breakdown provided in Figure 2. This comprehensive coverage ensures that The Pile encompasses a broad spectrum of datasets.

<img src="{{ site.baseurl }}/Lectures/S0-L06/images/piles/Componets.jpg" width="100%" height="100%">

![Data Sample](/Users/tempus/Library/CloudStorage/OneDrive-UniversityofVirginia/LLM/blog/Data Sample.jpg)



Furthermore, let's examine the structural statistics of the data. Firstly, the majority of documents in The Pile remain short, typically less than 10k bytes. However, there is also a long tail, indicating a small number of documents with lengths extending up to 60k bytes. Secondly, from a linguistic perspective, 97.4% of The Pile's dataset is in English. While The Pile aims to be multilingual-friendly, future expansion efforts will be necessary to achieve this goal.



![Length in bytes](/Users/tempus/Library/CloudStorage/OneDrive-UniversityofVirginia/LLM/blog/Length in bytes.jpg)



### Benchmark Models with The Pile

In this study, Bits per UTF-8 encoded byte (BPB) is utilized to evaluate perplexity, which measures the efficacy of AI in predicting the subsequent word. GPT2/3 models are employed to assess The Pile. Remarkably, as illustrated in the Figure, performance improves progressively with the expansion of model parameters, even when GPT2/3 models are not trained on The Pile. This finding, observed as early as 2020, underscores the significance of the study's results at the time of its publication.

<img src="{{ site.baseurl }}/Lectures/S0-L06/images/piles/Benchmark1.jpg" width="100%" height="100%">

### Benchmark on different Componet 

To further confirm how diversity improves the dataset's capability, we need to evaluate how the diverse dataset enhances performance on individual components. Unfortunately, due to resource limitations, the authors could not train GPT-3 from scratch on The Pile dataset. Instead, they opted for a proxy approach using the formula below:

The parameter ∆set represents the difference in performance of the GPT-3 model when evaluated on The Pile dataset (Lset) and its performance when evaluated on the OWT2 dataset (Lowt2). 

- Where:
  - LGPT3 is the performance metric of the GPT-3 model on The Pile dataset.
  - LGPT3_set is the performance metric of the GPT-3 model on the OWT2 dataset.
  - GPT2Pile_owt2 represents the performance difference between the GPT-2 model trained on The Pile dataset and the GPT-2 model trained on the OWT2 dataset.
  - GPT2Pile represents the performance of the GPT-2 model trained on The Pile dataset.
  - Lset is the intrinsic difficulty of understanding and generating text within The Pile dataset.
  - Lowt2 is the intrinsic difficulty of understanding and generating text within the OWT2 dataset

The term ∆set allows researchers to assess how much harder The Pile dataset is for GPT-3 compared to OWT2, while also considering the relative difficulty of tasks and the potential performance improvement achievable by training models specifically on The Pile dataset.

<img src="{{ site.baseurl }}/Lectures/S0-L06/images/piles/benchmark.jpg" width="100%" height="100%">


Observing the dotted line in the figure, which represents the average performance improvement, we notice significant enhancements in certain fields, such as DM Mathematics, Enron Emails, and others. This suggests that if GPT-3 were trained from scratch on The Pile dataset, its performance could potentially surpass the baseline model. Through these insights, we gain valuable understanding of the potential benefits of training language models on diverse datasets like The Pile.

### Evaluation 

To evaluate how the diversity from The Pile improves model training effectiveness, GPT-2 was trained on three different datasets, and the Bits per UTF-8 encoded byte (BPB) metric was employed for evaluation across the datasets. Refer to the table below for details.

<img src="{{ site.baseurl }}/Lectures/S0-L06/images/piles/Evalution.jpg" width="100%" height="100%">


From our observations, The Pile outperforms every dataset, with CC-100 showing minimal improvements compared to our baseline dataset, Raw CC. Notably, certain fields, such as Github, Stack Exchange, and DM Mathematics, exhibit significant improvements. This underscores the effectiveness of training datasets with diverse content in enhancing model training quality.

### More about the Pile

Another goal of this work is to address ethical and bias concerns in AI research, while also promoting and standardizing the practice of engaging with AI ethics. The paper's analysis delves into various perspectives, including topic distribution, inappropriate content, sensitive content (gender, religion, race), and data authority. Readers interested in these aspects can explore the paper to find topics of interest.

### Conclusion 

In conclusion, this work introduces a new open-source dataset that has been widely adopted in the research community since its release. The study demonstrates the dataset's capability enhancement by incorporating diverse categories of data through the evaluation process. Moreover, the work endeavors to address ethical and bias concerns in AI research, reflecting a commitment to responsible AI development.

**Mistral 7B**

**Why Mistral 7B**

- Outperforms Llama 2 13B on all benchmarks
- Outperforms Llama 1 34B on many benchmarks
- Approaches CodeLlama 7B performance on code, while remaining good at English tasks

 Here are essential components in Mistral 7B （ [Mistral / Mixtral Explained: Sliding Window Attention, Sparse Mixture of Experts, Rolling Buffer](https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2481s))

**Group-query attention**

Advantage:Accelerates the inference speed.Reduces the memory requirement during decoding, allowing for higher batch sizes hence higher throughput 

**Sliding Window Attention**

Using Stacked layers to attend information beyond the window size, where one hidden state can access up to h $\times$ k tokens.

<img src="{{ site.baseurl }}/Lectures/S0-L06/images/mistral7B/image0.jpg" width="100%" height="100%">

**Rolling Buffer Cache**

<img src="{{ site.baseurl }}/Lectures/S0-L06/images/mistral7B/image1.jpg" width="100%" height="100%">

- **Rolling Buffer Cache:** A mechanism to limit the memory usage of the attention mechanism by using a cache with a fixed size.
- **Fixed Cache Size:** The cache is set to a fixed size of *W*, storing only the most recent *W* key-value pairs.
- **Overwriting Mechanism:** When the timestep *i* exceeds *W*, older values are overwritten using the mod operation

**Pre-fill and chunking**

<img src="{{ site.baseurl }}/Lectures/S0-L06/images/mistral7B/image2.jpg" width="100%" height="100%">
- **Prompt Pre-filling:** The known parts of a prompt can be pre-processed to fill the (key, value) cache, which helps in quicker generation of subsequent tokens.
- **Large Prompt Handling:** For prompts too large to process at once, they can be divided into smaller segments, or "chunks".
- **Chunking Strategy:** The size of these chunks can be determined by a selected window size, which is optimal for the model's processing capabilities.

**Result:**

Here is Mistral 7B performance on different tasks (comparing to other open source LLM)

<img src="{{ site.baseurl }}/Lectures/S0-L06/images/mistral7B/image3.jpg" width="100%" height="100%">

<img src="{{ site.baseurl }}/Lectures/S0-L06/images/mistral7B/image4.jpg" width="100%" height="100%">

Mistral 7B performs equivalently to a Llama 2 that would be more than 3x its size. This is as much saved in memory and gained in throughput.

<img src="{{ site.baseurl }}/Lectures/S0-L06/images/mistral7B/image5.jpg" width="100%" height="100%">

**Finetuning Mistral 7B for Chat- Mistral 7B- Instruct**

<img src="{{ site.baseurl }}/Lectures/S0-L06/images/mistral7B/image6.jpg" width="100%" height="100%">

**Guardrials**

<img src="{{ site.baseurl }}/Lectures/S0-L06/images/mistral7B/image7.jpg" width="100%" height="100%">