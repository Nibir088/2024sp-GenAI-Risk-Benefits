---
layout: post
title: LLM multimodal / multilingual harm responses  
lecture: 
lectureVersion: next
extraContent: 
notes: team-4
video: team-3
tags:
- 1Basic
---

In this session, our readings cover: 

## Required Readings: 


### Cheating Suffix: Targeted Attack to Text-To-Image Diffusion Models with Multi-Modal Priors
+ Dingcheng Yang, Yang Bai, Xiaojun Jia, Yang Liu, Xiaochun Cao, Wenjian Yu
+ Diffusion models have been widely deployed in various image generation tasks, demonstrating an extraordinary connection between image and text modalities. However, they face challenges of being maliciously exploited to generate harmful or sensitive images by appending a specific suffix to the original prompt. Existing works mainly focus on using single-modal information to conduct attacks, which fails to utilize multi-modal features and results in less than satisfactory performance. Integrating multi-modal priors (MMP), i.e. both text and image features, we propose a targeted attack method named MMP-Attack in this work. Specifically, the goal of MMP-Attack is to add a target object into the image content while simultaneously removing the original object. The MMP-Attack shows a notable advantage over existing works with superior universality and transferability, which can effectively attack commercial text-to-image (T2I) models such as DALL-E 3. To the best of our knowledge, this marks the first successful attempt of transfer-based attack to commercial T2I models. Our code is publicly available at ....


### A Pilot Study of Query-Free Adversarial Attack against Stable Diffusion
+ https://ieeexplore.ieee.org/document/10208563
+ Despite the record-breaking performance in Text-to-Image (T2I) generation by Stable Diffusion, less research attention is paid to its adversarial robustness. In this work, we study the problem of adversarial attack generation for Stable Diffusion and ask if an adversarial text prompt can be obtained even in the absence of end-to-end model queries. We call the resulting problem ‘query-free attack generation’. To resolve this problem, we show that the vulnerability of T2I models is rooted in the lack of robustness of text encoders, e.g., the CLIP text encoder used for attacking Stable Diffusion. Based on such insight, we propose both untargeted and targeted query-free attacks, where the former is built on the most influential dimensions in the text embedding space, which we call steerable key dimensions. By leveraging the proposed attacks, we empirically show that only a five-character perturbation to the text prompt is able to cause the significant content shift of synthesized images using Stable Diffusion. Moreover, we show that the proposed target attack can precisely steer the diffusion model to scrub the targeted image content without causing much change in untargeted image content.


## More Readings: 


### Visual Instruction Tuning
+ Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee
+ Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.


### GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse
  + https://arxiv.org/abs/2401.01523

### Misusing Tools in Large Language Models With Visual Adversarial Examples
  + https://arxiv.org/abs/2310.03185

  

### Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned
  - https://arxiv.org/abs/2209.07858


<br /><br /><br />

# LLM Multimodal/Multilingual Harm Responses Blog

## A Pilot Study of Query-Free Adversarial Attack against Stable Diffusion
Inserting even small amounts of adversarial prompt can drastically alter results
<img src="{{ site.baseurl }}/Lectures/S0-L12/images/Bike_gen_ex_1.JPG" width="100%" height="100%">

### Diffusion Background
We've covered diffusion previously, but it is essentially the process of adding noise to an image one step at a time until it is nonsense (forward diffusion), and taking an image of pure noise and slowly removing the predicted noise to create an image (reverse diffusion). Most image generative models today use this reverse diffusion process, augmented with a text prompt. 

<img src="{{ site.baseurl }}/Lectures/S0-L12/images/Diffusion_ex_1.PNG" width="100%" height="100%">

Stable diffusion is the process of using a text prompt, via a text encoder such as clip, to guide the reverse diffusion process as mentioned previously. The text prompt is used as an input for the noise predictor that controls the de-noising process. 

<img src="{{ site.baseurl }}/Lectures/S0-L12/images/Diffusion_stable_2.PNG" width="100%" height="100%">

#### CLIP (Contrastive Language–Image Pre-training)
Dataset for describing images through negative association (what images are not about). This provides solid results and avoids cheating, making CLIP one of the more popular options for associating texts and images. CLIP is trained on the WebImage Text (WIT) image text pair set, with over 400M pairs.

<img src="{{ site.baseurl }}/Lectures/S0-L12/images/CLIP_ex_1.PNG" width="100%" height="100%">

### Generating Adversarial Perturbations
#### Query-based Attacks
Previous iterations of Text to Image (T2I) attacks use large numbers of model queries to find adversarial prompts. These are called Query-based attacks. A Query-free approach would be cheaper and more powerful, however

<img src="{{ site.baseurl }}/Lectures/S0-L12/images/Adversarial_Query_1.PNG" width="100%" height="100%">

#### Query-free Attacks
+ Assuming the attacker have access to the text encoder but not the diffusion model. Attack without executing the diffusion process which would take a high model query and computation cost.
+ Small perturbations on the text input of CLIP can lead to different CLIP scores, because of the sensitivity of the CLIP’s text embedding to text perturbations.
+ Query-free; Small(a five-character) perturbation; Attack on CLIP. Easy to get access to CLIP, and much less computationally expensive than attacking a full stable diffusion model. Also no risk of getting caught using the target model as you can run attacks locally against your own copy of CLIP as long as you know the target model uses it.

<img src="{{ site.baseurl }}/Lectures/S0-L12/images/Adversarial_CLIP_2.PNG" width="100%" height="100%">

### Attack Model
+ τθ(x) denote the text encoder of CLIP with parameters θ evaluated at the textual input x, find x’ that minimizing the cosine similarity between the text embeddings of x and x’.
+ x and x’ are independent from the diffusion model.
+ In this attack model, there is no target specified
+ Seek to minimize the cosine similarity of the encodings of x and x'
<img src="{{ site.baseurl }}/Lectures/S0-L12/images/Eq_1.PNG" width="100%" height="100%">

#### Targeted Attack
Targeting at removing the “yellow hat” (see figure from Query-free Attack section for reference)
Attack generated can be further refined towards a targeted attack purpose by guide the attack generator with
steerable key dimensions.
How to find key dimension? <img src="{{ site.baseurl }}/Lectures/S0-L12/images/Eq_2.PNG" width="100%" height="100%">
1. Generate n simple scenes and end with “with a yellow hat” s and n without
s1 = ‘A bird flew high in the sky with a yellow hat’ and s2 = ‘The sun set over the horizon with a yellow hat”
s′1 = ‘A bird flew high in the sky’ and s′2 = ‘The sun set over the horizon’.
2. Obtain the corresponding CLIP embeddings {τθ (si )} and {τθ (s′i )} .
The text embedding difference di = τθ (si ) − τθ (s′i ) can characterize the saliency of the adversary’s
intention-related sub-sentence
3. Find the binary vector I that represent the most influential dimensions
Ij = <img src="{{ site.baseurl }}/Lectures/S0-L12/images/Eq_3.PNG" width="100%" height="100%">

#### Attack Optimization Methods
Attack models are differentiable can use optimization methods
1. PGD(projected gradient descent): incorporates a perturbation budget (ϵ) and a step size (α) to control the amount and direction of perturbation x’ₜ₊₁ = Π(xₜ + α ⋅ sign(∇ₓJ(Θ, xₜ, y))), where, xₜ is the input at iteration t, α is the step size, ∇ₓJ(Θ, xₜ, y) is the gradient of the loss with respect to the input
2. Greedy search: a greedy search on the character candidate set to select the top 5 characters
3. Genetic algorithm: In each iteration, the genetic algorithm calls genetic operations such as mutation to generate new candidates
Details on implementation: [https://github.com/OPTML-Group/QF-Attack/blob/main/utils.py](https://github.com/OPTML-Group/QF-Attack/blob/main/utils.py)

### Experimental Evaluation
#### Experimental Set-up
+ Stable Diffusion model v1.4 as the victim model for image generation.
+ Attack methods details:
  + PGD: the base learning rate by 0.1 and the number of PGD steps by 100.
  + Genetic algorithm: the number of generation steps 50, the number of candidates per step 20, and the mutation rate 0.3
  + Targeted attack: ChatGPT to generate n = 10 sentences to characterize the steerable key dimensions and set ε = 0.9 to determine the influence mask I
#### Experiment Results
High level results:
<img src="{{ site.baseurl }}/Lectures/S0-L12/images/Experiment_QF_res1.PNG" width="100%" height="100%">

Untargeted results
<img src="{{ site.baseurl }}/Lectures/S0-L12/images/Adversarial_untargeted_3.PNG" width="100%" height="100%">

Targeted Results:
<img src="{{ site.baseurl }}/Lectures/S0-L12/images/Adversarial_targeted_4.PNG" width="100%" height="100%">

## Cheating Suffix: Targeted Text-to-Image Diffusion attack with Multi-Modal Priors
