---
layout: post
title: Survey AI Risk framework 
lecture: 
lectureVersion: current
extraContent: 
notes: team-4
video: team-4
tags:
- Mitigate
- Evaluate
---

In this session, our readings cover: 



## Required Readings: 

### TrustLLM: Trustworthiness in Large Language Models
+ https://arxiv.org/abs/2401.05561
+ Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and utility (i.e., functional effectiveness) are positively related. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones. Thirdly, it is important to note that some LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Finally, we emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. Knowing the specific trustworthy technologies that have been employed is crucial for analyzing their effectiveness.


### A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly
+ Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into "The Good" (beneficial LLM applications), "The Bad" (offensive applications), and "The Ugly" (vulnerabilities of LLMs and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs' potential to both bolster and jeopardize cybersecurity
+ https://arxiv.org/abs/2312.02003


## More Readings: 

### Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks
+ https://arxiv.org/abs/2212.14834
+ Large Language Models (LLMs) are swiftly advancing in architecture and capability, and as they integrate more deeply into complex systems, the urgency to scrutinize their security properties grows. This paper surveys research in the emerging interdisciplinary field of adversarial attacks on LLMs, a subfield of trustworthy ML, combining the perspectives of Natural Language Processing and Security. Prior work has shown that even safety-aligned LLMs (via instruction tuning and reinforcement learning through human feedback) can be susceptible to adversarial attacks, which exploit weaknesses and mislead AI systems, as evidenced by the prevalence of `jailbreak' attacks on models like ChatGPT a


### Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition
+ https://arxiv.org/abs/2311.16119
+ Large Language Models (LLMs) are deployed in interactive contexts with direct user engagement, such as chatbots and writing assistants. These deployments are vulnerable to prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and follow potentially malicious ones. Although widely acknowledged as a significant security threat, there is a dearth of large-scale resources and quantitative studies on prompt hacking. To address this lacuna, we launch a global prompt hacking competition, which allows for free-form human input attacks. We elicit 600K+ adversarial prompts against three state-of-the-art LLMs. We describe the dataset, which empirically verifies that current LLMs can indeed be manipulated via prompt hacking. We also present a comprehensive taxonomical ontology of the types of adversarial prompts.


### Even More: 

### ACL 2024 Tutorial: Vulnerabilities of Large Language Models to Adversarial Attacks

+ https://llm-vulnerability.github.io/ 


### Generative AI and ChatGPT: Applications, challenges, and AI-human collaboration
  + https://www.tandfonline.com/doi/full/10.1080/15228053.2023.2233814 
  

+ https://huggingface.co/blog?tag=ethics
  + https://huggingface.co/blog/ethics-diffusers 
  + https://huggingface.co/blog/model-cards 
  + https://huggingface.co/blog/us-national-ai-research-resource


### NIST AI RISK MANAGEMENT FRAMEWORK
  + https://www.nist.gov/itl/ai-risk-management-framework
  + https://airc.nist.gov/AI_RMF_Knowledge_Base/Playbook 
  + https://airc.nist.gov/AI_RMF_Knowledge_Base/Roadmap 
  +  EU AI Act / GDPR 


<br /><br /><br />

# AI Risk Framework Blog  

## Introduction and Background
+ Large language models have revolutionized natural language understanding and generation.
+ LLMs have gained the attention of in the security community, revealing security vulnerabilities and their potential in security-related tasks.
+ We will go over the intersection of LLMs with security and privacy.

### Exploring Crucial Security Research Questions
+ How do LLMs make a positive impact on security and privacy across diverse domains?
+ What potential risks and threats emerge from the utilization of LLMs within the realm of cybersecurity?
+ What vulnerabilities and weaknesses within LLMs, and how to defend against those threats?

### The Good, The Bad, and The Ugly of LLMs in Security
+ To comprehensively address the three main security-related questions, a meticulous literature review of 279 papers was conducted, categorizing them into three distinct groups. The paper, entitled "A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly" can be found at the link below:
<!-- insert Link to G/B/U paper here-->
#### The good: the papers highlighting security-beneficial applications.
+ LLMs have been used for secure coding, test case generation, vulnerable code detection, malicious code detection, and code fixing to name a few.
+ Most notably, researchers found LLM-based methods to outperform traditional approaches.
#### The bad: the papers exploring applications that could potentially exert adverse impacts on security.
+ LLMs also have offensive applications against security and privacy, categorizing them into five groups: 
+ Hardware-level attacks, OS-Level attacks, Software-level attacks, Network-level attacks, User-level attacks
  <!-- insert image from slide 8 here -->
#### The ugly: the papers focusing on the discussion of security vulnerabilities and potential defense mechanisms within LLMs.
1. AI-Inherent Vulnerabilities
+ Stem from the very nature and architecture of LLMs.
+ Adversarial attacks refer to strategies used to intentionally manipulate LLMs.
+ Inference attacks exploit unintended information leakage from responses.
+ Extraction attacks attempt to extract sensitive information from training data.
+ Instruction tuning attacks aim to provide explicit instructions during the fine-tuning process.
  <!-- insert image from slide 9 here --> 
2. Non-AI Inherent Vulnerabilities
+ Non-AI inherent attacks encompass external threats and new vulnerabilities LLMs might encounter.
+ Remote Code execution typically target LLMs to execute code arbitrarily.
+ Side channel attacks aim to leak information from the model.
+ Supply chain vulnerabilities refer to the risks that arise from using vulnerable components or services.
  <!-- insert image from slide 10 here -->

Vulnerabilities and Defenses Full Diagram
  <!-- insert image from slide 11 here -->

## Positive and Negative impacts on Security and Privacy
Continuing to cover the Good, Bad, Ugly paper, we now go further into the risks and benefits offered by AI.

### Benefits and Opportunities
**LLMs for Code Security**

`Code security lifecycle -> coding (C ) -> test case generation (TCG) ->  execution and monitoring (RE)`
1. Secure Coding (C)
+ Sandoval et al evaluated code written by student programmers when assisted by LLMs 
+ Finding; participants assisted by LLMs did not introduce new security risks
2. Test Case Generating (TCG)
+ Zhang et al. generated security tests (using ChatGPT-4.0) to assess the impact of vulnerable library dependencies on SW applications. 
+ Finding: LLMs could successfully generate tests that demonstrated various supply chain attacks, outperforming existing security test generators.

**Fuzzing (and its LLM based variations)**

Fuzzing is an industry standard technique: for generating test cases. It works by attempting to crash a system or trigger errors by supplying a large volume of random inputs.
By tracking which parts of the code are executed by these inputs, code coverage metrics can be calculated.

+ TitanFuzz - harnesses LLMs to generate input programs for fuzzing Deep Learning (DL) libraries (30-50% coverage, 41/65 bugs)
+ FuzzGPT - addresses the need for edge-case testing
+ WhiteFox - novel white-box compiler fuzzer that utilizes LLMs to test compiler optimizations.

An effective fuzzer generates semi-valid inputs that are "valid enough" in that they are not directly rejected by the parser, but do create unexpected behaviors deeper in the program and are "invalid enough" to expose corner cases that have not been properly dealt with.

**LLM in Running and Execution**

1. Vulnerability detection
+ Noever et. al.: GPT-4 identified approx. 4x vulnerabilities compared to traditional static code analyzers (e.g., Snyk and Fortify)
+ Moumita et al. applied LLMs for software vulnerability detection 
  + Finding: Higher False positive rate of LLM
+ Cheshkov et al. point out that the ChatGPT performed no better than a dummy classifier for both binary and multi-label classification tasks in code vulnerability detection
+ DefectHunter: combining LLMs with advanced models (e.g., Conformer) to identify software vulnerabilities effectively.

2. Malware Detection
+ Henrik Plate et . al. - LLM-based malware detection can complement human reviews but not replace them
  + Observation: use of simple tricks can also deceive the LLM’s assessments.
+ Apiiro - malicious code analysis tool using LLMs

3. Code fixing
+ ChatRepair: leverages PLMs for generating patches without dependency on bug-fixing datasets.

Note: Malware is the threat while vulnerabilities are exploitable risks and unsecured entry points that can be leveraged by threat actors

**Findings of LLM in Code Security**

+ LLM-based methods outperform traditional approaches (advantages include higher code coverage, higher detecting accuracy, less cost etc.). 
+ LLM-based methods do not surpass SOTA approaches (4 authors)
  + Reason: tendency to produce both high false negatives and false positives when detecting vulnerabilities or bugs.
+ ChatGPT is the predominant LLM extensively employed














### Risks and Threats
